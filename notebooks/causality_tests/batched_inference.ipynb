{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "from collections import OrderedDict\n",
    "\n",
    "from dataset import llama_v2_prompt\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "device = \"cuda\"\n",
    "torch_device = \"cuda\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = 'yours_here'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-13b-chat-hf\", token=access_token, padding_side='left')\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-13b-chat-hf\", token=access_token)\n",
    "model.half().cuda();\n",
    "model.eval();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the probe weights here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from src.probes import LinearProbeClassification\n",
    "\n",
    "from src.intervention_utils import return_classifier_dict\n",
    "\n",
    "classifier_type = LinearProbeClassification\n",
    "classifier_directory = \"probe_checkpoints/controlling_probe\"\n",
    "return_user_msg_last_act = True\n",
    "include_inst = True\n",
    "layer_num = None\n",
    "mix_scaler = False\n",
    "residual_stream = True\n",
    "logistic = True\n",
    "sklearn = False\n",
    "\n",
    "classifier_dict = return_classifier_dict(classifier_directory,\n",
    "                                         classifier_type, \n",
    "                                         chosen_layer=layer_num,\n",
    "                                         mix_scaler=mix_scaler,\n",
    "                                         logistic=logistic,\n",
    "                                         sklearn=sklearn,\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batched Intervention code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from baukit import TraceDict\n",
    "from torch import nn\n",
    "\n",
    "\n",
    "if '<pad>' not in tokenizer.get_vocab():\n",
    "    tokenizer.add_special_tokens({\"pad_token\":\"<pad>\"})\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))\n",
    "model.config.pad_token_id = tokenizer.pad_token_id\n",
    "assert model.config.pad_token_id == tokenizer.pad_token_id, \"The model's pad token ID does not match the tokenizer's pad token ID!\"\n",
    "\n",
    "residual = True\n",
    "def optimize_one_inter_rep(inter_rep, layer_name, target, probe,\n",
    "                           N=4, normalized=False):\n",
    "    global first_time\n",
    "    tensor = (inter_rep.clone()).to(torch_device).requires_grad_(True)\n",
    "    rep_f = lambda: tensor\n",
    "    target_clone = target.clone().to(torch_device).to(torch.float)\n",
    "\n",
    "    cur_input_tensor = rep_f().clone().detach()\n",
    "\n",
    "    if normalized:\n",
    "        cur_input_tensor = rep_f() + target_clone.view(1, -1) @ probe.proj[0].weight * N * 100 / rep_f().norm() \n",
    "    else:\n",
    "        cur_input_tensor = rep_f() + target_clone.view(1, -1) @ probe.proj[0].weight * N\n",
    "    return cur_input_tensor.clone()\n",
    "\n",
    "\n",
    "def edit_inter_rep_multi_layers(output, layer_name):\n",
    "    if residual:\n",
    "        layer_num = layer_name[layer_name.rfind(\"model.layers.\") + len(\"model.layers.\"):]\n",
    "    else:\n",
    "        layer_num = layer_name[layer_name.rfind(\"model.layers.\") + len(\"model.layers.\"):layer_name.rfind(\".mlp\")]\n",
    "    layer_num = int(layer_num)\n",
    "    probe = classifier_dict[attribute][layer_num + 1]\n",
    "    cloned_inter_rep = output[0][:,-1].unsqueeze(0).detach().clone().to(torch.float)\n",
    "    with torch.enable_grad():\n",
    "        cloned_inter_rep = optimize_one_inter_rep(cloned_inter_rep, layer_name, \n",
    "                                                  cf_target, probe,\n",
    "                                                  lr=100, max_epoch=1e5, \n",
    "                                                  loss_func=nn.BCELoss(),\n",
    "                                                  simplified=True,\n",
    "                                                  N=N,\n",
    "                                                  normalized=False)\n",
    "    output[0][:,-1] = cloned_inter_rep.to(torch.float16)\n",
    "    return output\n",
    "\n",
    "\n",
    "def collect_responses_batched(prompts, modified_layer_names, edit_function, batch_size=5, rand=None):\n",
    "    print(modified_layer_names)\n",
    "    responses = []\n",
    "    for i in tqdm(range(0, len(prompts), batch_size)): \n",
    "        \n",
    "        message_lists = [[{\"role\": \"user\", \n",
    "                         \"content\": prompt},\n",
    "                        ] for prompt in prompts[i:i+batch_size]]\n",
    "\n",
    "        # Transform the message list into a prompt string\n",
    "        formatted_prompts = [llama_v2_prompt(message_list) for message_list in message_lists]\n",
    "        \n",
    "        with TraceDict(model, modified_layer_names, edit_output=edit_function) as ret:\n",
    "            with torch.no_grad():\n",
    "                inputs = tokenizer(formatted_prompts, return_tensors='pt', padding=True).to('cuda')\n",
    "                tokens = model.generate(**inputs,\n",
    "                                        max_new_tokens=768,\n",
    "                                        do_sample=False,\n",
    "                                        temperature=generation_temperature,\n",
    "                                        top_p=generation_top_p,\n",
    "                                       )\n",
    "                \n",
    "        output = [tokenizer.decode(seq, skip_special_tokens=True).split('[/INST]')[1] for seq in tokens]\n",
    "        responses.extend(output)\n",
    "\n",
    "    return responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalized=False\n",
    "# Sampling hyperparameters\n",
    "generation_temperature = 0\n",
    "generation_top_p = 1\n",
    "\n",
    "N = 8 # Intervention Strength\n",
    "\n",
    "which_layers = [] # Which layer/s to intervene\n",
    "from_idx = 20 # Hyperparameter\n",
    "to_idx = 30 # Hyperparameter\n",
    "residual = True # Set True\n",
    "for name, module in model.named_modules():\n",
    "    if residual and name!= \"\" and name[-1].isdigit():\n",
    "        layer_num = name[name.rfind(\"model.layers.\") + len(\"model.layers.\"):]\n",
    "        if from_idx <= int(layer_num) < to_idx:\n",
    "            which_layers.append(name)\n",
    "    elif (not residual) and name.endswith(\".mlp\"):\n",
    "        layer_num = name[name.rfind(\"model.layers.\") + len(\"model.layers.\"):name.rfind(\".mlp\")]\n",
    "        if from_idx <= int(layer_num) < to_idx:\n",
    "            which_layers.append(name)\n",
    "modified_layer_names = which_layers\n",
    "        \n",
    "attribute = \"your choice (gender, age, etc.)\" # which attribute to intervene"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example on gender"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's use gender as an example\n",
    "# Gender has two subcategories so the cf target will have a length of 2\n",
    "cf_target = [0, 0]\n",
    "# The element at index 0 indicate whether we want to intervene on the male attribute\n",
    "# and the element at index 1 indicate whether we want to intervene on the female attribute\n",
    "\n",
    "# Let's say we want to intervene on the male attribute\n",
    "# set element at index 0 to 1\n",
    "cf_target[0] = 1\n",
    "cf_target = torch.Tensor([cf_target])\n",
    "\n",
    "# and we want the strength to be 8\n",
    "N = 8\n",
    "\n",
    "# We want to modify layers 20 to 30\n",
    "modified_layer_names = which_layers\n",
    "\n",
    "batch_size = 2\n",
    "# and we have an array of questions\n",
    "questions = [\"Can you give me some outfits suggestions? I am going to attend my friend's birthday party tonight\", \n",
    "             \"What birthday gifts should I bring to my friends?\",]\n",
    "results = collect_responses_batched(questions, modified_layer_names, edit_inter_rep_multi_layers, batch_size=batch_size, rand=None)\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Print out the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(questions)):\n",
    "    text = f\"USER: {questions[i]}\\n\\n\"\n",
    "    text += \"-\" * 50 + \"\\n\"\n",
    "    text += f\"Intervened:\\n\"\n",
    "    text += f\"CHATBOT: {results[i]}\"\n",
    "    text += \"\\n\\n\" + \"-\" * 50 + \"\\n\"\n",
    "    \n",
    "    print(text)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
