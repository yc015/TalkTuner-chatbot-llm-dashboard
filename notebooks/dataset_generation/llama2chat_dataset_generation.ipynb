{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29e6f761-1459-4ac3-9762-a3ba53d9b4d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "from transformers import BertTokenizerFast\n",
    "import torch.nn.functional as F\n",
    "\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "tic, toc = (time.time, time.time)\n",
    "\n",
    "from dataset import split_conversation, llama_v2_prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a34f849-3f9b-4eff-9957-61a2fdd8d37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "access_token = 'yours_here'\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"meta-llama/Llama-2-13b-chat-hf\", access_token=access_token)\n",
    "model = AutoModelForCausalLM.from_pretrained(\"meta-llama/Llama-2-13b-chat-hf\", access_token=access_token)\n",
    "model.half().cuda();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0900f118-11c6-455e-ac05-3afe1c70ab48",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_whitespaces_before_word(document, word):\n",
    "    # Split the document into lines using newline as a delimiter\n",
    "    lines = document.split('\\n')\n",
    "\n",
    "    # Initialize a new list to store the modified lines\n",
    "    modified_lines = []\n",
    "\n",
    "    # Iterate through the lines\n",
    "    for line in lines:\n",
    "        # Split each line into words using whitespace as a delimiter\n",
    "        words = line.split()\n",
    "\n",
    "        # Initialize a new list to store the modified words in the line\n",
    "        modified_words = []\n",
    "\n",
    "        # Flag to indicate if we have found the specific word in the line\n",
    "        found_word = False\n",
    "\n",
    "        # Iterate through the words in the line\n",
    "        for w in words:\n",
    "            if w == word:\n",
    "                # If the word matches the specific word, set the flag to True\n",
    "                found_word = True\n",
    "\n",
    "            # Append the current word to the modified_words list\n",
    "            modified_words.append(w)\n",
    "\n",
    "        # If the specific word was found in the line and it's not the first word, remove whitespaces before it\n",
    "        if found_word and len(modified_words) > 1:\n",
    "            modified_words[-1] = modified_words[-1].rstrip()\n",
    "\n",
    "        # Join the modified_words back into a single line with spaces between them\n",
    "        modified_line = ' '.join(modified_words)\n",
    "\n",
    "        # Append the modified line to the modified_lines list\n",
    "        modified_lines.append(modified_line)\n",
    "\n",
    "    # Join the modified_lines back into a single string with newline characters between them\n",
    "    modified_document = '\\n'.join(modified_lines)\n",
    "\n",
    "    return modified_document"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0e9b9e-fbac-41b9-a30f-048a65e95cb7",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "# Age Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47151ad0-4e9a-4a00-8c2a-a1c5bdb0175c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "torch.manual_seed(75241239)\n",
    "\n",
    "i = 0\n",
    "num_samples = 'your_number_here'\n",
    "year_ranges = [\"below 12 years old\", \"between 13 to 17 years old\", \"between 18 to 64 years old\",\n",
    "               \"above 65 years old\"]\n",
    "ages = [\"child\", \"adolescent\", \"adult\", \"older adult\"]\n",
    "\n",
    "output_dir = 'datasets_llama2/age'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "for (age, year_range) in zip(ages, year_ranges):\n",
    "    while i < num_samples:\n",
    "        prompt = f\"\"\"\\\n",
    "        SYSTEM: Generate a conversation between a user and an AI assistant. This human user is a {age} who is {year_range}. Make sure the conversation reflect this user's age. You may or may not include the user's age directly in the conversation. Make the conversation sound natural. The user's response should start with 'HUMAN:', and the AI assistant's response should start with 'ASSISTANT:'. Mark the end of the generated conversation with '<End of Conversation>'\\\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = tokenizer(prompt, return_tensors='pt').to('cuda')\n",
    "            tokens = model.generate(\n",
    "             **inputs,\n",
    "             max_new_tokens=2048,\n",
    "             do_sample=True,\n",
    "             temperature=1.0,\n",
    "             top_p=0.8,\n",
    "             # repetition_penalty=1.15,\n",
    "            )\n",
    "        # print(tokenizer.decode(tokens[0], skip_special_tokens=True))\n",
    "\n",
    "        output = tokenizer.decode(tokens[0], skip_special_tokens=True)\n",
    "        start_tag = \"HUMAN: \"\n",
    "        end_tag = \"<End of Conversation>\"\n",
    "        conversation_start = output.find(start_tag)\n",
    "        conversation = output[conversation_start:]\n",
    "        conversation_end = conversation.find(end_tag)\n",
    "        conversation = conversation[:conversation_end]\n",
    "        conversation = conversation.strip()\n",
    "        word_to_remove_spaces = \"HUMAN\"\n",
    "        conversation = remove_whitespaces_before_word(conversation, word_to_remove_spaces)\n",
    "        word_to_remove_spaces = \"ASSISTANT\"\n",
    "        conversation = remove_whitespaces_before_word(conversation, word_to_remove_spaces)\n",
    "\n",
    "        # with open(f'datasets_llama2/age/conversation_{i}_age_{age}.txt', 'w') as f:\n",
    "        with open(f'{output_dir}/conversation_{i}_age_{age}.txt', 'w') as f:\n",
    "            f.write(conversation)\n",
    "        f.close()\n",
    "        try:\n",
    "            user_msgs, ai_msgs = split_conversation(conversation)\n",
    "            messages_dict = []\n",
    "\n",
    "            for user_msg, ai_msg in zip(user_msgs, ai_msgs):\n",
    "                messages_dict.append({'content': user_msg, 'role': 'user'})\n",
    "                messages_dict.append({'content': ai_msg, 'role': 'assistant'})\n",
    "            text = llama_v2_prompt(messages_dict)\n",
    "            i += 1\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    i = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c34eb6-876f-4e3c-b50d-7103536b86fb",
   "metadata": {},
   "source": [
    "# Gender Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55f71d5b-f702-46a3-82a3-cbb9beb329ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(75241239)\n",
    "i = 0\n",
    "num_samples = 'your_number_here'\n",
    "\n",
    "output_dir = 'datasets_llama2/gender'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir) \n",
    "\n",
    "for gender in [\"male\", \"female\"]:\n",
    "    while i < num_samples:\n",
    "        prompt = f\"\"\"\\\n",
    "        SYSTEM: Generate a conversation between a human user and an AI assistant. This human user is a {gender}. Make sure the conversation reflect this user's gender. You may or may not include the user's gender directly in the conversation. Make the conversation sound natural. The user's response should start with 'HUMAN:', and the AI assistant's response should start with 'ASSISTANT:'. Mark the end of the generated conversation with '<End of Conversation>'\\\n",
    "        \"\"\"\n",
    "\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = tokenizer(prompt, return_tensors='pt').to('cuda')\n",
    "            tokens = model.generate(\n",
    "             **inputs,\n",
    "             max_new_tokens=2048,\n",
    "             do_sample=True,\n",
    "             temperature=1.0,\n",
    "             top_p=0.8,\n",
    "             # repetition_penalty=1.15,\n",
    "            )\n",
    "        # print(tokenizer.decode(tokens[0], skip_special_tokens=True))\n",
    "\n",
    "        output = tokenizer.decode(tokens[0], skip_special_tokens=True)\n",
    "        start_tag = \"HUMAN: \"\n",
    "        end_tag = \"<End of Conversation>\"\n",
    "        conversation_start = output.find(start_tag)\n",
    "        conversation = output[conversation_start:]\n",
    "        conversation_end = conversation.find(end_tag)\n",
    "        conversation = conversation[:conversation_end]\n",
    "        conversation = conversation.strip()\n",
    "        word_to_remove_spaces = \"HUMAN\"\n",
    "        conversation = remove_whitespaces_before_word(conversation, word_to_remove_spaces)\n",
    "        word_to_remove_spaces = \"ASSISTANT\"\n",
    "        conversation = remove_whitespaces_before_word(conversation, word_to_remove_spaces)\n",
    "\n",
    "        with open(f'{output_dir}/conversation_{i}_age_{gender}.txt', 'w') as f:\n",
    "            f.write(conversation)\n",
    "        f.close()\n",
    "        \n",
    "        try:\n",
    "            user_msgs, ai_msgs = split_conversation(conversation)\n",
    "            messages_dict = []\n",
    "\n",
    "            for user_msg, ai_msg in zip(user_msgs, ai_msgs):\n",
    "                messages_dict.append({'content': user_msg, 'role': 'user'})\n",
    "                messages_dict.append({'content': ai_msg, 'role': 'assistant'})\n",
    "            text = llama_v2_prompt(messages_dict)\n",
    "            i += 1\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    i = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d298c66-6478-4a25-af7a-c716ec1ed1bf",
   "metadata": {},
   "source": [
    "# Socioeconomic Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29d95ce0-ad9c-4f62-8f9c-6bd8357d3160",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "num_samples = 'your_number_here'\n",
    "\n",
    "output_dir = 'datasets_llama2/socioeconomic'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "socioeco_status = [\"low\", \"middle\", \"high\"]\n",
    "for socioeco in socioeco_status:\n",
    "    while i < num_samples:\n",
    "        prompt = f\"\"\"\\\n",
    "        SYSTEM: Generate a conversation between a human user and an AI assistant. The socioeconomic status of this human user is {socioeco}. Make sure the conversation reflect this user's socioeconomic status. You may or may not include the user's socioeconomic status directly in the conversation. Make the conversation sound natural. The user's response should start with 'HUMAN:', and the AI assistant's response should start with 'ASSISTANT:'. Mark the end of the generated conversation with '<End of Conversation>'\\\n",
    "        \"\"\"\n",
    "\n",
    "        with torch.no_grad():\n",
    "            inputs = tokenizer(prompt, return_tensors='pt').to('cuda')\n",
    "            tokens = model.generate(\n",
    "             **inputs,\n",
    "             max_new_tokens=2048,\n",
    "             do_sample=True,\n",
    "             temperature=1.0,\n",
    "             top_p=0.8,\n",
    "             # repetition_penalty=1.15,\n",
    "            )\n",
    "        # print(tokenizer.decode(tokens[0], skip_special_tokens=True))\n",
    "\n",
    "        output = tokenizer.decode(tokens[0], skip_special_tokens=True)\n",
    "        start_tag = \"HUMAN: \"\n",
    "        end_tag = \"<End of Conversation>\"\n",
    "        conversation_start = output.find(start_tag)\n",
    "        conversation = output[conversation_start:]\n",
    "        conversation_end = conversation.find(end_tag)\n",
    "        conversation = conversation[:conversation_end]\n",
    "        conversation = conversation.strip()\n",
    "        word_to_remove_spaces = \"HUMAN\"\n",
    "        conversation = remove_whitespaces_before_word(conversation, word_to_remove_spaces)\n",
    "        word_to_remove_spaces = \"ASSISTANT\"\n",
    "        conversation = remove_whitespaces_before_word(conversation, word_to_remove_spaces)\n",
    "\n",
    "        with open(f'{output_dir}/conversation_{i}_socioeco_{socioeco}.txt', 'w') as f:\n",
    "            f.write(conversation)\n",
    "        f.close()\n",
    "        \n",
    "        try:\n",
    "            user_msgs, ai_msgs = split_conversation(conversation)\n",
    "            messages_dict = []\n",
    "\n",
    "            for user_msg, ai_msg in zip(user_msgs, ai_msgs):\n",
    "                messages_dict.append({'content': user_msg, 'role': 'user'})\n",
    "                messages_dict.append({'content': ai_msg, 'role': 'assistant'})\n",
    "            text = llama_v2_prompt(messages_dict)\n",
    "            i += 1\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    i = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59384ce5-5e76-4ab2-aaa7-97ecbca34b5f",
   "metadata": {},
   "source": [
    "# Education Conversations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed8d22bb-7684-46e8-82dd-7710760a55b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 0\n",
    "num_samples = 'your_number_here'\n",
    "educations = [\"some schooling (pre-high school)\", \"high school education\", \n",
    "              \"college and more\"]\n",
    "acrys = [\"someschool\", \n",
    "        \"highschool\", \"collegemore\"]\n",
    "\n",
    "output_dir = 'datasets_llama2/education'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.makedirs(output_dir)\n",
    "\n",
    "for education, acry in zip(educations, acrys):\n",
    "    while i < num_samples:\n",
    "        prompt = f\"\"\"\\\n",
    "        SYSTEM: Generate a conversation between a human user and an AI assistant. The education level of this human user is {education}. Make sure the content of conversation reflect this user's education level. You may or may not include the user's education level directly in the conversation. Note that the AI Assistant does not know the user's education level at the beginning. Make the conversation sound natural. The user's response should start with 'HUMAN:', and the AI assistant's response should start with 'ASSISTANT:'. Mark the end of the generated conversation with '<End of Conversation>'\\\n",
    "        \"\"\"\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            inputs = tokenizer(prompt, return_tensors='pt').to('cuda')\n",
    "            tokens = model.generate(\n",
    "             **inputs,\n",
    "             max_new_tokens=2048,\n",
    "             do_sample=True,\n",
    "             temperature=1.0,\n",
    "             top_p=0.8,\n",
    "             # repetition_penalty=1.15,\n",
    "            )\n",
    "\n",
    "        output = tokenizer.decode(tokens[0], skip_special_tokens=True)\n",
    "        start_tag = \"HUMAN: \"\n",
    "        end_tag = \"<End of Conversation>\"\n",
    "        conversation_start = output.find(start_tag)\n",
    "        conversation = output[conversation_start:]\n",
    "        conversation_end = conversation.find(end_tag)\n",
    "        conversation = conversation[:conversation_end]\n",
    "        conversation = conversation.strip()\n",
    "        word_to_remove_spaces = \"HUMAN\"\n",
    "        conversation = remove_whitespaces_before_word(conversation, word_to_remove_spaces)\n",
    "        word_to_remove_spaces = \"ASSISTANT\"\n",
    "        conversation = remove_whitespaces_before_word(conversation, word_to_remove_spaces)\n",
    "\n",
    "        try:\n",
    "            with open(f'{output_dir}/conversation_{i}_education_{acry}.txt', 'w') as f:\n",
    "                f.write(conversation)\n",
    "            f.close()\n",
    "        except:\n",
    "            with open(f'{output_dir}/conversation_{i}_education_{acry}.txt', 'w', encoding='utf-8') as f:\n",
    "                f.write(conversation)\n",
    "            f.close()\n",
    "        try:\n",
    "            user_msgs, ai_msgs = split_conversation(conversation)\n",
    "            messages_dict = []\n",
    "\n",
    "            for user_msg, ai_msg in zip(user_msgs, ai_msgs):\n",
    "                messages_dict.append({'content': user_msg, 'role': 'user'})\n",
    "                messages_dict.append({'content': ai_msg, 'role': 'assistant'})\n",
    "            text = llama_v2_prompt(messages_dict)\n",
    "            i += 1\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "    i = 0"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:.conda-diffusion-viz-gpu]",
   "language": "python",
   "name": "conda-env-.conda-diffusion-viz-gpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
